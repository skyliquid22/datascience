{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By Ahmed Elshayeb (aelshayeb@jacobs-alumni.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FilterMatrix class functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This description is partially based on the __scikit-learn User Guide on Covariance estimation__ [available here](https://scikit-learn.org/stable/modules/covariance.html#robust-covariance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FilterMatrix class includes the implementations of functions for different ways to adjust Covariance matrices.\n",
    "\n",
    "The following algorithms currently implemented: \n",
    "\n",
    "- De-noising Covariance/Correlation Matrix\n",
    "  - Spectral Clustering De-noising Method\n",
    "  - Hierarchical Clustering De-noising Method\n",
    "- Transforming covariance matrix to correlation matrix and back\n",
    "\n",
    "This Notebook will describe the above algorithms as well as provide use cases and analysis of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De-noising Covariance/Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Clustering De-noising Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea behind spectral clustering is to remove the noise-related eigenvalues from an empirical correlation matrix, the method in which this is achieved is by setting the eigenvalues which are below the theoretical value to their average value, they are set to zero in an attempt to remove the effects of those eigenvalues that are consistent with the null hypothesis of uncorrelated random variables.\n",
    "\n",
    "Let us consider $n$ independent random variables with finite variance and $T$ records each. Random matrix\n",
    "theory allows to prove that in the $\\lim\\limits_{n \\to \\infty} T$, with a fixed ratio $Q = T/n \\geq 1$, the\n",
    "eigenvalues of the sample correlation matrix cannot be larger than\n",
    "\n",
    "$$ \\lambda_{max} = \\sigma^2(1 + \\frac{1}{Q} + 2\\sqrt{\\frac{1}{Q}})$$\n",
    "\n",
    "where $\\sigma^2 = 1$ for correlation matrices, once achieved we set any eignevalues above this threshold to $0$.\n",
    "For example, we have a set of 5 eigenvalues sorted in the descending order ( $\\lambda_1$ ... $\\lambda_5$ ),\n",
    "3 of which are below the maximum theoretical value, then we set\n",
    "\n",
    "$$ \\lambda_3^{NEW} = \\lambda_4^{NEW} = \\lambda_5^{NEW} = 0$$\n",
    "\n",
    "- Eigenvalues above the maximum theoretical value are left intact.\n",
    "\n",
    "$$\\lambda_1^{NEW} = \\lambda_1^{OLD}$$\n",
    "\n",
    "$$\\lambda_2^{NEW} = \\lambda_2^{OLD}$$\n",
    "\n",
    "- The new set of eigenvalues with the set of eigenvectors is used to obtain the new de-noised correlation matrix. $\\tilde{C}$ is the de-noised correlation matrix, $W$ is the eigenvectors matrix, and $\\Lambda$ is the diagonal matrix with new eigenvalues.\n",
    "\n",
    "$$\\tilde{C} = W \\Lambda W$$\n",
    "\n",
    "- To rescale $\\tilde{C}$ so that the main diagonal consists of 1s the following transformation is made. This is how the final $C_{denoised}$ is obtained.\n",
    "\n",
    "$$C_{denoised} = \\tilde{C} [(diag[\\tilde{C}])^\\frac{1}{2}(diag[\\tilde{C}])^{\\frac{1}{2}'}]^{-1}$$\n",
    "\n",
    "- The new correlation matrix is then transformed back to the new de-noised covariance matrix.\n",
    "\n",
    "The process of de-noising the covariance matrix is described in a paper by _Potter M._, _J.P. Bouchaud_, _L. Laloux_ __“Financial applications of random matrix theory: Old laces and new pieces.”__  [available here](https://arxiv.org/abs/physics/0507111)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datascience.filter.filter import Filtermatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EEM</th>\n",
       "      <th>EWG</th>\n",
       "      <th>TIP</th>\n",
       "      <th>EWJ</th>\n",
       "      <th>EFA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-01-02</th>\n",
       "      <td>49.273335</td>\n",
       "      <td>35.389999</td>\n",
       "      <td>106.639999</td>\n",
       "      <td>52.919998</td>\n",
       "      <td>78.220001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-03</th>\n",
       "      <td>49.716667</td>\n",
       "      <td>35.290001</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>53.119999</td>\n",
       "      <td>78.349998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-04</th>\n",
       "      <td>48.223331</td>\n",
       "      <td>34.599998</td>\n",
       "      <td>106.970001</td>\n",
       "      <td>51.759998</td>\n",
       "      <td>76.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-07</th>\n",
       "      <td>48.576668</td>\n",
       "      <td>34.630001</td>\n",
       "      <td>106.949997</td>\n",
       "      <td>51.439999</td>\n",
       "      <td>76.650002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-08</th>\n",
       "      <td>48.200001</td>\n",
       "      <td>34.389999</td>\n",
       "      <td>107.029999</td>\n",
       "      <td>51.320000</td>\n",
       "      <td>76.220001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  EEM        EWG         TIP        EWJ        EFA\n",
       "Date                                                              \n",
       "2008-01-02  49.273335  35.389999  106.639999  52.919998  78.220001\n",
       "2008-01-03  49.716667  35.290001  107.000000  53.119999  78.349998\n",
       "2008-01-04  48.223331  34.599998  106.970001  51.759998  76.570000\n",
       "2008-01-07  48.576668  34.630001  106.949997  51.439999  76.650002\n",
       "2008-01-08  48.200001  34.389999  107.029999  51.320000  76.220001"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the data\n",
    "stock_prices = pd.read_csv('../datascience/dataset/stock_prices.csv', parse_dates=True, index_col='Date', dayfirst=True)\n",
    "stock_prices = stock_prices.dropna(axis=1)\n",
    "\n",
    "# Leaving only 5 stocks in the dataset, so the differences between the \n",
    "# calculated covariance matrices would be easy to observe.\n",
    "stock_prices = stock_prices.iloc[:, :5]\n",
    "stock_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can calculate the returns from the prices\n",
    "stock_returns = stock_prices.pct_change()\n",
    "stock_returns.dropna(inplace=True, how='all')\n",
    "\n",
    "# Relation of number of observations T to the number of variables N (T/N)\n",
    "tn_relation = stock_prices.shape[0] / stock_prices.shape[1]\n",
    "\n",
    "# Get the positive definate covariance and correlation matricies \n",
    "cov_matrix = stock_returns.cov() \n",
    "corr_matrix = stock_returns.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spectral Clustering De-noised Сovariance matrix is:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EEM</th>\n",
       "      <th>EWG</th>\n",
       "      <th>TIP</th>\n",
       "      <th>EWJ</th>\n",
       "      <th>EFA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EEM</th>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EWG</th>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>-0.000085</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIP</th>\n",
       "      <td>-0.000095</td>\n",
       "      <td>-0.000085</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EWJ</th>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EFA</th>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.000278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          EEM       EWG       TIP       EWJ       EFA\n",
       "EEM  0.000466  0.000416 -0.000095  0.000329  0.000360\n",
       "EWG  0.000416  0.000372 -0.000085  0.000294  0.000322\n",
       "TIP -0.000095 -0.000085  0.000019 -0.000067 -0.000073\n",
       "EWJ  0.000329  0.000294 -0.000067  0.000232  0.000254\n",
       "EFA  0.000360  0.000322 -0.000073  0.000254  0.000278"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate class Filtermatrix\n",
    "filt = Filtermatrix()\n",
    "\n",
    "# Filter the covariance matrix using Spectral Clustering De-noising method\n",
    "cov_matrix_spectral = filt.denoise_covariance(cov_matrix, tn_relation)\n",
    "\n",
    "# Transforming De-noised Covariance from np.array to pd.DataFrame\n",
    "cov_matrix_spectral = pd.DataFrame(cov_matrix_spectral, index=cov_matrix.index, columns=cov_matrix.columns)\n",
    "\n",
    "# Outputting the result\n",
    "print('The Spectral Clustering De-noised Сovariance matrix is:')\n",
    "cov_matrix_spectral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering De-noising Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Clustering, unlike K-means Clustering, does not create multiple clusters of identical size, nor does it\n",
    "require a pre-defined number of clusters. Of the two different types of hierarchical clustering - Agglomerative and\n",
    "Divisive - Agglomerative, or bottom-up clustering is used here.\n",
    "\n",
    "Agglomerative Clustering assigns each observation to its own individual cluster before iteratively joining the two\n",
    "most similar clusters. This process repeats until only a singular cluster remains.\n",
    "\n",
    "Given a positive empirical correlation matrix, $C$ generated using $n$ features, the procedure given below\n",
    "returns as an output a rooted tree and a filtered correlation matrix $C^<$ of elements $c^<_{ij}$.\n",
    "\n",
    "First, set $C = C^<$. \n",
    "\n",
    "Then, beginning with the most highly correlated features (clusters) $h$ and $k \\in C$ and the correlation\n",
    "between them, $c_{hk}$, one sets the elements $c^<_{ij} = c^<_{ji} = c_{hk}$.\n",
    "\n",
    "The matrix $C^<$ is then redefined such that:\n",
    "\n",
    "$$\\begin{cases} c^<_{qj} = f(c^<_{hj}, c^<_{kj}) & where \\ j \\notin h \\ and \\ j \\notin k \\\\ c^<_{ij} = c^<_{ij} & otherwise \\end{cases}$$\n",
    "\n",
    "where $f(c^<_{hj}, c^<_{kj})$ is any distance metric. In effect, merging the clusters $h$ and $k$.\n",
    "These steps are then completed for the next two most similar clusters, and are repeated for a total\n",
    "of $n-1$ iterations; until only a single cluster remains.\n",
    "\n",
    "There are a few methods to use with Hierarchical Clustering for calculating the distance metric, here are 3 of them:\n",
    "\n",
    "- **Single** $d(u,v) = min(dist(u[i], v[j]))$ for all points $i$ in cluster $u$ and $j$ in cluster $v$. This is also known as the Nearest Point Algorithm.\n",
    "\n",
    "- **Complete** $d(u,v) = max(dist(u[i], v[j]))$ for all points $i$ in cluster $u$ and $j$ in cluster $v$. This is also known by the Farthest Point Algorithm or Voor Hees Algorithm\n",
    "\n",
    "- **Average** $d(u,v) = \\displaystyle\\sum_{ij} \\frac{d(u[i], v[j])}{|u| * |v|}$ for all points $i$ in cluster $|u|$ and $|j|$ in cluster $u$ and $v$, respectively. This is also called the UPGMA algorithm.\n",
    "\n",
    "The process of de-noising the covariance/correlation matrix using Hierarchichal Clustering is described in a paper by _Michele Tumminello_, _Fabrizio Lillo_, and _Rosario N. Mantegna_  __“Correlation, hierarchies, and networks in financial markets”__  [available here](https://arxiv.org/pdf/0809.4615.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The Hierarchical Clustering requires a positive correlation matrix\n",
    "# Compute the square of the correlation matrix to get rid of the negative values\n",
    "corr_positive = np.array(corr_matrix ** 2)\n",
    "\n",
    "# Filter the covariance matrix using the Hierarchical Clustering De-noising method\n",
    "corr_matrix_hierarchical = filt.filter_corr_hierarchical(corr_positive, method='complete', draw_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Figure above shows the Dendogram of the hierarchichal clustered matrix where X-axis includes the column ID, and_\n",
    "\n",
    "_the Y-axis shows the alpha(filtered) value of the correlations for each pair_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the functions implemented in the FilterMatrix class, related to different ways of adjusting the Covariance matrix. Also, it shows how the corresponding functions from the this library can be used and how the outputs can be analyzed.\n",
    "\n",
    "Key takeaways from the notebook:\n",
    "- The Spectral Clustering De-noising Method calculates the eigenvalues of the correlation matrix and eliminates the ones that are lower than the theoretically estimated ones, as they are caused by noise.\n",
    "- The Hierarchical Clustering De-noising Method is used to filter empirical correlation matrices using Agglomerative Clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
